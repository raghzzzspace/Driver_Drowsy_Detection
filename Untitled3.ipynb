{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raghzzzspace/Driver_Drowsy_Detection/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXNTh0NdU59K"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'closed-and-open-eyes:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5784816%2F9504621%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240929%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240929T144529Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D75bef85f4452ca8a02bf091668930186ec8109d29cee1f9e0802a1eac3d1630dc249d75b60684ab493125be624c8fd996d58673c1f329ec815308d17d85f3370e0a42c4eb9a275a871c5b23a6abca04d62e4fb1ad100f5e74693f2248455efb003362df8b04dd958fec483704b989b9f0dca97244cf5e847ffbeb719e51c71383f9b9e35d11e11b9336f4fb6abfa4718baf1e35a07352b9913b6600a1428dac204419b037cb5c09587404038a6365b6a769793b31fab85abba057ebca75e033657b112db9630cf8162062c0b6954c8a5826e0a6e70b74dc43f290328acee28495d69e1ea62c736b6fffce7e8c0d1cf9a46e025e2e4ccfd31047e57ebe982dd95'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Source directories\n",
        "source_dir_drowsy = '/kaggle/input/closed-and-open-eyes/authentic_ddd/drowsy/'\n",
        "source_dir_non_drowsy = '/kaggle/input/closed-and-open-eyes/authentic_ddd/notdrowsy/'  # Assuming the non-drowsy images are in this folder\n",
        "\n",
        "# Destination directories\n",
        "base_dir = '/kaggle/working/driver_drowsiness_dataset/'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "train_drowsy_dir = os.path.join(train_dir, 'drowsy')\n",
        "test_drowsy_dir = os.path.join(test_dir, 'drowsy')\n",
        "train_non_drowsy_dir = os.path.join(train_dir, 'notdrowsy')\n",
        "test_non_drowsy_dir = os.path.join(test_dir, 'notdrowsy')\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(train_drowsy_dir, exist_ok=True)\n",
        "os.makedirs(test_drowsy_dir, exist_ok=True)\n",
        "os.makedirs(train_non_drowsy_dir, exist_ok=True)\n",
        "os.makedirs(test_non_drowsy_dir, exist_ok=True)\n",
        "\n",
        "# Split ratio for train/test\n",
        "split_ratio = 0.8\n",
        "\n",
        "# Helper function to split files into train/test\n",
        "def split_data(source_dir, train_dir, test_dir, split_ratio=0.8):\n",
        "    # Get all files in source directory\n",
        "    all_files = os.listdir(source_dir)\n",
        "    random.shuffle(all_files)  # Shuffle the files for randomness\n",
        "\n",
        "    # Split into train/test\n",
        "    split_index = int(len(all_files) * split_ratio)\n",
        "    train_files = all_files[:split_index]\n",
        "    test_files = all_files[split_index:]\n",
        "\n",
        "    # Move files to the corresponding train and test directories\n",
        "    for file_name in train_files:\n",
        "        src_file = os.path.join(source_dir, file_name)\n",
        "        dest_file = os.path.join(train_dir, file_name)\n",
        "        shutil.copy(src_file, dest_file)  # Copying the file to the train directory\n",
        "\n",
        "    for file_name in test_files:\n",
        "        src_file = os.path.join(source_dir, file_name)\n",
        "        dest_file = os.path.join(test_dir, file_name)\n",
        "        shutil.copy(src_file, dest_file)  # Copying the file to the test directory\n",
        "\n",
        "# Step 1: Split Drowsy images into train and test\n",
        "split_data(source_dir_drowsy, train_drowsy_dir, test_drowsy_dir, split_ratio)\n",
        "\n",
        "# Step 2: Split Non-Drowsy images into train and test\n",
        "split_data(source_dir_non_drowsy, train_non_drowsy_dir, test_non_drowsy_dir, split_ratio)\n",
        "\n",
        "print(f\"Data successfully split into train and test directories at {base_dir}\")"
      ],
      "metadata": {
        "id": "YvxzR-K9U-qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "id": "n8CDv1mXVR_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16, DenseNet121\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, Concatenate, GlobalAveragePooling2D, Input, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras import mixed_precision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Enable mixed precision for faster training on modern GPUs\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# Directories for training and validation\n",
        "train_dir = '/kaggle/working/driver_drowsiness_dataset/train'\n",
        "test_dir = '/kaggle/working/driver_drowsiness_dataset/test'\n",
        "\n",
        "# Image dimensions (both models expect 224x224 input images)\n",
        "img_width, img_height = 224, 224\n",
        "batch_size = 32\n",
        "epochs = 10  # Increased for better fine-tuning\n",
        "\n",
        "# Preprocessing the images\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,  # Normalize pixel values to [0,1]\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Training data generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Test data generator\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Shared Input Layer for both VGG16 and DenseNet\n",
        "input_tensor = Input(shape=(img_width, img_height, 3))\n",
        "\n",
        "# Load the VGG16 model with pretrained weights, without the top layers\n",
        "vgg16_model = VGG16(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
        "\n",
        "# Load the DenseNet121 model with pretrained weights, without the top layers\n",
        "densenet_model = DenseNet121(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
        "\n",
        "# Fine-tune by unfreezing the last few layers of each model\n",
        "for layer in vgg16_model.layers[-4:]:  # Unfreeze the last 4 layers\n",
        "    layer.trainable = True\n",
        "\n",
        "for layer in densenet_model.layers[-4:]:  # Unfreeze the last 4 layers\n",
        "    layer.trainable = True\n",
        "\n",
        "# VGG16 branch\n",
        "vgg16_output = vgg16_model.output\n",
        "vgg16_output = GlobalAveragePooling2D()(vgg16_output)\n",
        "\n",
        "# DenseNet branch\n",
        "densenet_output = densenet_model.output\n",
        "densenet_output = GlobalAveragePooling2D()(densenet_output)\n",
        "\n",
        "# Concatenate the outputs of VGG16 and DenseNet\n",
        "combined = Concatenate()([vgg16_output, densenet_output])\n",
        "\n",
        "# Add fully connected layers after the concatenation\n",
        "x = Dense(512, activation='relu')(combined)\n",
        "x = BatchNormalization()(x)  # Add batch normalization to stabilize learning\n",
        "x = Dropout(0.5)(x)  # Dropout to reduce overfitting\n",
        "x = Dense(1, activation='sigmoid', dtype='float32')(x)  # Ensure output is float32 despite mixed precision\n",
        "\n",
        "# Combine the VGG16 and DenseNet models\n",
        "model = Model(inputs=input_tensor, outputs=x)\n",
        "\n",
        "# Compile the model with a learning rate scheduler and mixed precision\n",
        "optimizer = Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Save the best model during training\n",
        "model_checkpoint = ModelCheckpoint('vgg16_densenet_drowsiness_best.keras', save_best_only=True, monitor='val_loss')\n",
        "\n",
        "# Reduce learning rate when a metric has stopped improving\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
        "\n",
        "# Train the model with the callbacks\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=test_generator,\n",
        "    validation_steps=test_generator.samples // batch_size,\n",
        "    callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
        ")\n",
        "\n",
        "# Save the final model\n",
        "model.save('vgg16_densenet_drowsiness_final.keras')\n",
        "\n",
        "print(\"Optimized VGG16 and DenseNet model trained and saved successfully!\")\n",
        "\n",
        "# Plot accuracy and loss\n",
        "def plot_metrics(history):\n",
        "    # Accuracy plot\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Call the function to plot the metrics\n",
        "plot_metrics(history)"
      ],
      "metadata": {
        "id": "JjzUcx2KVDvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2  # OpenCV for image processing\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "import os\n",
        "import requests\n",
        "import threading\n",
        "import random\n",
        "\n",
        "# Load the trained model\n",
        "\n",
        "model = load_model('vgg16_densenet_drowsiness_best.keras')\n",
        "\n",
        "def thingspeak_post(num):\n",
        "    # try:\n",
        "    #     threading.Timer(1, thingspeak_post).start()\n",
        "    # except RuntimeError:\n",
        "    #     print(\"Cannot start a new thread. Interpreter is shutting down.\")\n",
        "    #     return\n",
        "    val=num\n",
        "    URL = 'https://api.thingspeak.com/update'\n",
        "    KEY = 'Q2EZEWQ4Z4D1L37V'\n",
        "    payload = {'api_key': KEY, 'field1': val}\n",
        "\n",
        "    try:\n",
        "        response = requests.get(URL, params=payload)\n",
        "        if response.status_code == 200:\n",
        "            print(f\"Successfully sent data: {payload}\")\n",
        "        else:\n",
        "            print(f\"Failed to send data. Status code: {response.status_code}\")\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "# Function to preprocess input image\n",
        "def preprocess_image(image):\n",
        "    # Load the image\n",
        "    # image = cv2.imread(image_path)\n",
        "\n",
        "    # Check if the image was loaded correctly\n",
        "    if image is None:\n",
        "        raise ValueError(f\"Image not found or unable to load\")\n",
        "\n",
        "    # Resize the image to the input size expected by the model\n",
        "    image = cv2.resize(image, (224, 224))  # VGG16 and DenseNet expect 224x224 images\n",
        "    image = image.astype(\"float32\") / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "    # Convert the image to an array and add a batch dimension\n",
        "    image = img_to_array(image)\n",
        "    image = np.expand_dims(image, axis=0)  # Create a batch of 1\n",
        "    return image\n",
        "\n",
        "\n",
        "def predict_drowsiness(image):\n",
        "    image = preprocess_image(image)\n",
        "    prediction = model.predict(image)\n",
        "    print(prediction)\n",
        "    return prediction[0][0]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  info = [[0 for _ in range(30)] for _ in range(3)]\n",
        "  thingspeak_post(0)\n",
        "  v = cv2.VideoCapture('WhatsApp Video 2024-09-30 at 15.03.50.mp4')\n",
        "  if not v.isOpened():\n",
        "      print(\"Error: Could not open video.\")\n",
        "      exit()\n",
        "  count=1\n",
        "  sec_cnt=0\n",
        "  frame_cnt=0\n",
        "  alert=0\n",
        "  slp_check=0\n",
        "  # Loop through the video frames\n",
        "  while True:\n",
        "      ret, image = v.read()\n",
        "      if not ret:\n",
        "          print(\"End of video or cannot fetch the frame.\")\n",
        "          break\n",
        "\n",
        "      print(count)\n",
        "      res=predict_drowsiness(image)\n",
        "      if res < 0.025:\n",
        "            print(\"Drowsiness Detected\",res)\n",
        "            slp_check=slp_check+1-info[sec_cnt][frame_cnt]\n",
        "            info[sec_cnt][frame_cnt]=1\n",
        "      else:\n",
        "            print(\"Driver is Awake\",res)\n",
        "            slp_check=slp_check-info[sec_cnt][frame_cnt]\n",
        "            info[sec_cnt][frame_cnt]=0\n",
        "\n",
        "      if slp_check>=65 and not alert:\n",
        "        print('raise_alarm')\n",
        "        thingspeak_post(1)\n",
        "        alert=1\n",
        "      elif alert and slp_check<65:\n",
        "        thingspeak_post(0)\n",
        "        alert=0\n",
        "\n",
        "\n",
        "\n",
        "      count += 1\n",
        "      if frame_cnt<29:\n",
        "        frame_cnt+=1\n",
        "      else :\n",
        "        frame_cnt=0\n",
        "\n",
        "        if sec_cnt<2:\n",
        "          sec_cnt+=1\n",
        "        else:\n",
        "          sec_cnt=0\n",
        "\n",
        "v.release()\n"
      ],
      "metadata": {
        "id": "PjgAPy0jVTpt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}